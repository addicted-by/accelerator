from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Dict, Iterable, Optional

import torch
from omegaconf import OmegaConf

from accelerator.runtime.datamodule.datamodule import DataModule
from accelerator.tools.analysis.model_analysis import NodeInterpreter, StatsRouter, trace_model
from accelerator.tools.analysis.stats import StatsConfig, TensorStatsCollector, save_tensor_stats
from accelerator.utilities.hydra_utils import instantiate


class ModelAnalysisCLI:
    """Commands for model statistics analysis."""

    # ------------------------------------------------------------------
    def config(self, model_name: str, cfg_name: Optional[str] = None) -> str:
        """Generate per-node statistics configuration for ``model_name``.

        Parameters
        ----------
        model_name:
            Name of the model configuration located in ``configs/model``.
        cfg_name:
            Optional output configuration name. Defaults to ``model_name``.

        Returns
        -------
        Path to the generated JSON configuration file.
        """
        model_cfg_path = Path("configs/model") / f"{model_name}.yaml"
        model_cfg = OmegaConf.load(model_cfg_path)
        model = instantiate(model_cfg.model_core)

        gm, registry = trace_model(model)
        default_cfg = StatsConfig(channel_dim=1)
        cfg_dict: Dict[str, Any] = {
            "activation_config": {
                "channel_dim": default_cfg.channel_dim,
                "collectors": [c.__name__ for c in default_cfg.collectors],
            },
            "gradient_config": {
                "channel_dim": default_cfg.channel_dim,
                "collectors": [c.__name__ for c in default_cfg.collectors],
            },
            "activation_configs": {
                node.name: {
                    "channel_dim": default_cfg.channel_dim,
                    "collectors": [c.__name__ for c in default_cfg.collectors],
                }
                for node in registry
            },
            "gradient_configs": {
                node.name: {
                    "channel_dim": default_cfg.channel_dim,
                    "collectors": [c.__name__ for c in default_cfg.collectors],
                }
                for node in registry
            },
        }
        out_path = Path(f"{cfg_name or model_name}.json")
        with out_path.open("w") as f:
            json.dump(cfg_dict, f, indent=2)
        return str(out_path)

    # ------------------------------------------------------------------
    def collect(
        self,
        model_name: str,
        dataset_name: str,
        config_path: str,
        filter_layers: Optional[Iterable[str]] = None,
        exclude_layers: Optional[Iterable[str]] = None,
        output: Optional[str] = None,
    ) -> str:
        """Collect per-node statistics for a model and dataset.

        Parameters
        ----------
        model_name: Name of the model configuration in ``configs/model``.
        dataset_name: Name of the datamodule configuration in ``configs/datamodule``.
        config_path: Path to the JSON configuration generated by ``config``.
        filter_layers: Optional patterns to select layers for statistics collection.
        exclude_layers: Optional patterns to exclude layers from collection.
        output: Optional path for resulting statistics JSON file.

        Returns
        -------
        Path to the saved statistics file.
        """
        model_cfg = OmegaConf.load(Path("configs/model") / f"{model_name}.yaml")
        data_cfg = OmegaConf.load(Path("configs/datamodule") / f"{dataset_name}.yaml")

        model = instantiate(model_cfg.model_core)

        # Ensure dataset root is set
        if data_cfg.get("datasets"):
            for ds_cfg in data_cfg.datasets.values():
                ds_cfg.setdefault("root", "./data")
                ds_cfg.setdefault("download", True)
        data_module = DataModule.initialize_from_config(data_cfg)
        loader = data_module.get_dataloader("train")
        if loader is None:
            # Fallback to any available dataloader
            loader = next(iter(data_module.dataloaders.values()))

        with open(config_path) as f:
            cfg_json = json.load(f)

        from accelerator.tools.analysis import stats as stats_mod

        def _cfg_from_dict(d: Dict[str, Any]) -> StatsConfig:
            collectors = tuple(getattr(stats_mod, name) for name in d.get("collectors", []))
            channel_dim = d.get("channel_dim", 1)
            return StatsConfig(channel_dim=channel_dim, collectors=collectors or (StatsConfig.collectors))

        activation_cfg = _cfg_from_dict(cfg_json.get("activation_config", {}))
        gradient_cfg = _cfg_from_dict(cfg_json.get("gradient_config", {}))
        act_configs = {k: _cfg_from_dict(v) for k, v in cfg_json.get("activation_configs", {}).items()}
        grad_configs = {k: _cfg_from_dict(v) for k, v in cfg_json.get("gradient_configs", {}).items()}

        collector = TensorStatsCollector(
            activation_config=activation_cfg,
            gradient_config=gradient_cfg,
            activation_configs=act_configs,
            gradient_configs=grad_configs,
        )

        gm, _ = trace_model(model)
        included = set(act_configs.keys()) | set(grad_configs.keys())
        router = StatsRouter(
            collector,
            nodes=included if included else None,
            filter_layers=filter_layers,
            exclude_layers=exclude_layers,
        )
        interpreter = NodeInterpreter(gm, router)

        for batch in loader:
            if isinstance(batch, (list, tuple)):
                inputs = batch[0]
            else:
                inputs = batch
            if isinstance(inputs, list):
                result = interpreter.run(*inputs)
            else:
                result = interpreter.run(inputs)
            loss = result.sum() if isinstance(result, torch.Tensor) else result[0].sum()
            loss.backward()

        input_activations, activations, gradients = collector.compute()
        out_file = output or "analysis_stats.json"
        save_tensor_stats(activations, gradients, out_file, input_activations)
        return out_file


class AnalyzeCLI:
    def __init__(self) -> None:
        self.model = ModelAnalysisCLI()
